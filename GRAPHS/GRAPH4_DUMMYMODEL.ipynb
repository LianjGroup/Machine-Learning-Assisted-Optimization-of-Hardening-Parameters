{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramConfig = {\n",
    "    'c1': {'lowerBound': 0, 'upperBound': 1, 'exponent': 1.0, 'name': 'W', 'unit': 'dimensionless', 'type': 'hardening'}, \n",
    "    'c2': {'lowerBound': 0, 'upperBound': 2, 'exponent': 1000.0, 'name': 'K', 'unit': 'MPa', 'type': 'yielding'}, \n",
    "    'c3': {'lowerBound': 0, 'upperBound': 1, 'exponent': 0.1, 'name': 'e0', 'unit': 'dimensionless', 'type': 'hardening'}, \n",
    "    'c4': {'lowerBound': 0, 'upperBound': 1, 'exponent': 1.0, 'name': 'n', 'unit': 'dimensionless', 'type': 'hardening'}, \n",
    "    'c5': {'lowerBound': 0, 'upperBound': 2, 'exponent': 1000.0, 'name': 'sigma_y', 'unit': 'MPa', 'type': 'yielding'}, \n",
    "    'c6': {'lowerBound': 0, 'upperBound': 1, 'exponent': 1000.0, 'name': 'sigma_sat', 'unit': 'MPa', 'type': 'hardening'}, \n",
    "    'c7': {'lowerBound': 0, 'upperBound': 1, 'exponent': 1000.0, 'name': 'b', 'unit': 'dimensionless', 'type': 'hardening'}\n",
    "}\n",
    "\n",
    "geometries = ['NDBR50', 'NDBR6', 'CHD6']\n",
    "\n",
    "yieldingIndices = {'NDBR50': 200, 'NDBR6': 200, 'CHD6': 1200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'combined_interpolated_param_to_geom_FD_Curves_smooth.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sankoktas/Desktop/coeproject3/GRAPHS/GRAPH4_DUMMYMODEL.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sankoktas/Desktop/coeproject3/GRAPHS/GRAPH4_DUMMYMODEL.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mASSETS\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mPART2_ASSTFUNCT\u001b[39;00m \u001b[39mimport\u001b[39;00m\u001b[39m*\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sankoktas/Desktop/coeproject3/GRAPHS/GRAPH4_DUMMYMODEL.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Load the data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sankoktas/Desktop/coeproject3/GRAPHS/GRAPH4_DUMMYMODEL.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m combined_interpolated_params_to_geoms_FD_Curves_smooth \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mcombined_interpolated_param_to_geom_FD_Curves_smooth.npy\u001b[39;49m\u001b[39m'\u001b[39;49m, allow_pickle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sankoktas/Desktop/coeproject3/GRAPHS/GRAPH4_DUMMYMODEL.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m targetCurves \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mtargetCurves.npy\u001b[39m\u001b[39m'\u001b[39m, allow_pickle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sankoktas/Desktop/coeproject3/GRAPHS/GRAPH4_DUMMYMODEL.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Define the function for the RMSE loss\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'combined_interpolated_param_to_geom_FD_Curves_smooth.npy'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from botorch.acquisition.multi_objective import qExpectedHypervolumeImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.utils.multi_objective.box_decompositions import NondominatedPartitioning\n",
    "from botorch.acquisition.multi_objective.objective import IdentityMCMultiOutputObjective\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ASSETS.PART5_GUARD import*\n",
    "from ASSETS.PART2_ASSTFUNCT import*\n",
    "\n",
    "# Load the data\n",
    "combined_interpolated_params_to_geoms_FD_Curves_smooth = np.load('combined_interpolated_param_to_geom_FD_Curves_smooth.npy', allow_pickle=True).tolist()\n",
    "targetCurves = np.load('targetCurves.npy', allow_pickle=True).tolist()\n",
    "\n",
    "# Define the function for the RMSE loss\n",
    "def lossFD(targetDisp, targetForce, simForce):\n",
    "    return torch.sqrt(torch.mean((simForce - targetForce)**2))\n",
    "\n",
    "# Calculate losses and prepare data for model\n",
    "params = []\n",
    "losses = []\n",
    "\n",
    "for param_tuple, geom_to_simCurves in combined_interpolated_params_to_geoms_FD_Curves_smooth.items():\n",
    "    #print(param_tuple)\n",
    "    params.append([value for param, value in param_tuple])\n",
    "    # The minus sign is because BOTORCH tries to maximize objectives, but we want to minimize the loss\n",
    "    loss_iter = []\n",
    "    for geometry in geometries:\n",
    "        yieldingIndex = yieldingIndices[geometry]\n",
    "        loss_iter.append(- lossFD(\n",
    "            torch.tensor(targetCurves[geometry][\"displacement\"][yieldingIndex:]), \n",
    "            torch.tensor(targetCurves[geometry][\"force\"][yieldingIndex:]), \n",
    "            torch.tensor(geom_to_simCurves[geometry][\"force\"][yieldingIndex:])\n",
    "        ))\n",
    "    losses.append(loss_iter)\n",
    "    \n",
    "# Convert your data to the tensor(float 64)\n",
    "X = torch.tensor(params, dtype=torch.float64)\n",
    "Y = torch.stack([torch.tensor(loss, dtype=torch.float64) for loss in losses])\n",
    "\n",
    "# Normalize X to have range of [0, 1]\n",
    "minmax_scaler = MinMaxScaler()\n",
    "X_normalized = torch.tensor(minmax_scaler.fit_transform(X.numpy()), dtype=torch.float64)\n",
    "\n",
    "# Standardize Y to have zero mean and unit variance\n",
    "standard_scaler = StandardScaler()\n",
    "Y_standardized = torch.tensor(standard_scaler.fit_transform(Y.numpy()), dtype=torch.float64)\n",
    "\n",
    "# Define the bounds of the search space\n",
    "lower_bounds = torch.tensor([paramConfig[param]['lowerBound'] * paramConfig[param]['exponent'] for param in paramConfig.keys()]).float()\n",
    "upper_bounds = torch.tensor([paramConfig[param]['upperBound'] * paramConfig[param]['exponent'] for param in paramConfig.keys()]).float()\n",
    "\n",
    "\n",
    "lower_bounds = minmax_scaler.transform(lower_bounds.reshape(1, -1)).squeeze()\n",
    "upper_bounds = minmax_scaler.transform(upper_bounds.reshape(1, -1)).squeeze()\n",
    "\n",
    "bounds = torch.tensor(np.array([lower_bounds, upper_bounds])).float()\n",
    "\n",
    "# Initialize model\n",
    "model = SingleTaskGP(X_normalized, Y_standardized)\n",
    "mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "fit_gpytorch_model(mll)\n",
    "\n",
    "# **Reference Point**\n",
    "\n",
    "# qEHVI requires specifying a reference point, which is the lower bound on the objectives used for computing hypervolume. \n",
    "# In this tutorial, we assume the reference point is known. In practice the reference point can be set \n",
    "# 1) using domain knowledge to be slightly worse than the lower bound of objective values, \n",
    "# where the lower bound is the minimum acceptable value of interest for each objective, or \n",
    "# 2) using a dynamic reference point selection strategy.\n",
    "\n",
    "ref_point = Y_standardized.min(dim=0).values - 0.01\n",
    "\n",
    "partitioning = NondominatedPartitioning(ref_point=ref_point, Y=Y_standardized)\n",
    "acq_func = qExpectedHypervolumeImprovement(\n",
    "    model=model,\n",
    "    partitioning=partitioning,\n",
    "    ref_point=ref_point,\n",
    "    objective=IdentityMCMultiOutputObjective(),\n",
    ")\n",
    "\n",
    "\n",
    "# Optimize the acquisition function\n",
    "candidates, _ = optimize_acqf(\n",
    "    acq_function=acq_func,\n",
    "    bounds=bounds,\n",
    "    q=1, #q: This is the number of points to sample in each step. \n",
    "    num_restarts=10, # num_restarts: This is the number of starting points for the optimization.\n",
    "    raw_samples=1000, # raw_samples: This is the number of samples to draw when initializing the optimization\n",
    ")\n",
    "\n",
    "# Unnormalize the candidates\n",
    "candidates = minmax_scaler.inverse_transform(candidates.detach().numpy())\n",
    "\n",
    "#converting to dictionary\n",
    "next_param_dicts = [{param: value.item() for param, value in zip(paramConfig.keys(), next_param)} for next_param in candidates]\n",
    "\n",
    "print(next_param_dicts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
