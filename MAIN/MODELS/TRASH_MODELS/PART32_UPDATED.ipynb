{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100], Loss: 0.0661\n",
      "Predicted parameters for test data point with the minimum validation loss (Epoch 91):\n",
      "[6.0767514e-01 7.4054230e+02 6.5964244e-02 4.8257834e-01 1.1596931e+03\n",
      " 1.5319138e+02 3.0968250e+02]\n",
      "\n",
      "Loss at min_val_loss_index (91): 0.0611\n",
      "R-squared value on the test set: -5293998.0702\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# Load your data\n",
    "X_data = pd.read_csv(\"MODEL_DATA/NEWDATA/newData_combined_FD.csv\").values\n",
    "Y_data = pd.read_csv(\"MODEL_DATA/NEWDATA/newData_expanded_realHardParam.csv\").values\n",
    "\n",
    "# Data scaling\n",
    "input_scaler = MinMaxScaler()\n",
    "X_data_scaled = input_scaler.fit_transform(X_data)\n",
    "\n",
    "target_scaler = MinMaxScaler()\n",
    "Y_data_scaled = target_scaler.fit_transform(Y_data)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(X_data_scaled, Y_data_scaled, test_size=0.2, random_state=42)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# DataLoader for the training set\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(Y_train, dtype=torch.float32))\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# DataLoader for the validation set\n",
    "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(Y_val, dtype=torch.float32))\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)  # No need to shuffle for evaluation\n",
    "\n",
    "# DataLoader for the test set\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(Y_test, dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)  # No need to shuffle for evaluation\n",
    "\n",
    "# Define the updated neural network model\n",
    "class UpdatedNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, lambda_reg):\n",
    "        super(UpdatedNeuralNetwork, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, hidden_sizes[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(hidden_sizes[i]))\n",
    "            layers.append(nn.Dropout(0.5))\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def l2_regularization(self):\n",
    "        l2_reg = 0.0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                l2_reg += torch.norm(layer.weight)\n",
    "        return self.lambda_reg * l2_reg\n",
    "\n",
    "# Custom loss function with regularization\n",
    "class CustomMSELoss(nn.Module):\n",
    "    def __init__(self, lambda_reg):\n",
    "        super(CustomMSELoss, self).__init__()\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def forward(self, outputs, targets, model):\n",
    "        mse_loss = nn.MSELoss()(outputs, targets)\n",
    "        return mse_loss + model.l2_regularization()\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_sizes = [128, 128, 64]\n",
    "output_size = Y_train.shape[1]\n",
    "lambda_reg = 0.001\n",
    "\n",
    "# Instantiate the updated model\n",
    "model = UpdatedNeuralNetwork(input_size, hidden_sizes, output_size, lambda_reg)\n",
    "criterion = CustomMSELoss(lambda_reg)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Initialize minimum validation loss and index\n",
    "min_val_loss = float('inf')\n",
    "min_val_loss_index = -1\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets, model)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_inputs = torch.tensor(X_val, dtype=torch.float32)\n",
    "        val_outputs = model(val_inputs)\n",
    "        loss_val = criterion(val_outputs, torch.tensor(Y_val, dtype=torch.float32), model)\n",
    "\n",
    "    # Check if current validation loss is the minimum\n",
    "    if loss_val < min_val_loss:\n",
    "        min_val_loss = loss_val\n",
    "        min_val_loss_index = epoch\n",
    "\n",
    "    # Optional: Print loss every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set and print the parameters for the data point with the minimum validation loss\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_inputs = torch.tensor(X_test, dtype=torch.float32)\n",
    "    test_outputs = model(test_inputs)\n",
    "    test_predictions = target_scaler.inverse_transform(test_outputs.numpy())\n",
    "\n",
    "# Print the parameters for the data point with the minimum validation loss\n",
    "if min_val_loss_index != -1:\n",
    "    parameters_min_loss = test_predictions[min_val_loss_index * len(X_test) // num_epochs]\n",
    "    print(f\"Predicted parameters for test data point with the minimum validation loss (Epoch {min_val_loss_index + 1}):\")\n",
    "    print(parameters_min_loss)\n",
    "    print()\n",
    "else:\n",
    "    print(\"No minimum validation loss found.\")\n",
    "\n",
    "print(f'Loss at min_val_loss_index ({min_val_loss_index + 1}): {loss_val.item():.4f}')\n",
    "\n",
    "# Convert predictions to NumPy array\n",
    "test_predictions_np = test_predictions\n",
    "\n",
    "# Inverse transform the scaled predictions to get the original scale\n",
    "predicted_parameters_test = target_scaler.inverse_transform(test_predictions_np)\n",
    "\n",
    "# Ensure the shapes match for calculating R-squared\n",
    "y_test_subset = Y_data[len(X_train) + len(X_val):]  # Extract the corresponding subset of the original Y_data\n",
    "y_test_subset = y_test_subset[:len(predicted_parameters_test)]\n",
    "\n",
    "# Calculate R-squared value\n",
    "r2_value = r2_score(y_test_subset, predicted_parameters_test)\n",
    "print(f\"R-squared value on the test set: {r2_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.0682\n",
      "Epoch [200/1000], Loss: 0.0518\n",
      "Epoch [300/1000], Loss: 0.0555\n",
      "Epoch [400/1000], Loss: 0.0551\n",
      "Epoch [500/1000], Loss: 0.0421\n",
      "Epoch [600/1000], Loss: 0.0396\n",
      "Epoch [700/1000], Loss: 0.0396\n",
      "Epoch [800/1000], Loss: 0.0435\n",
      "Epoch [900/1000], Loss: 0.0447\n",
      "Epoch [1000/1000], Loss: 0.0344\n",
      "c1: Predicted = 5.14e-01, Ideal = 5.00e-01, Close = 92.59%\n",
      "c2: Predicted = 1.33e+03, Ideal = 1.30e+03, Close = 94.96%\n",
      "c3: Predicted = 2.34e-14, Ideal = 2.30e-14, Close = 88.40%\n",
      "c4: Predicted = 7.19e-02, Ideal = 7.50e-02, Close = 91.52%\n",
      "c5: Predicted = 7.44e+02, Ideal = 7.73e+02, Close = 91.59%\n",
      "c6: Predicted = 1.02e+03, Ideal = 1.04e+03, Close = 93.06%\n",
      "c7: Predicted = 7.76e+01, Ideal = 7.39e+01, Close = 92.59%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Define ideal parameters\n",
    "ideal_params = {\n",
    "    'c1': 0.5,\n",
    "    'c2': 1300,\n",
    "    'c3': 2.3e-14,\n",
    "    'c4': 0.075,\n",
    "    'c5': 773.18,\n",
    "    'c6': 1039.37,\n",
    "    'c7': 73.94\n",
    "}\n",
    "\n",
    "# Load the data\n",
    "x_data_path = 'MODEL_DATA/NEWDATA/newData_combined_FD.csv'  \n",
    "y_data_path = 'MODEL_DATA/NEWDATA/newData_expanded_realHardParam.csv' \n",
    "\n",
    "x_data = pd.read_csv(x_data_path)\n",
    "y_data = pd.read_csv(y_data_path)\n",
    "\n",
    "# Use a subset of the data for faster training\n",
    "subset_percentage = 0.99\n",
    "x_data_subset, _, y_data_subset, _ = train_test_split(x_data, y_data, test_size=1 - subset_percentage, random_state=42)\n",
    "\n",
    "# Data scaling\n",
    "input_scaler = MinMaxScaler()\n",
    "X_data_scaled = input_scaler.fit_transform(x_data_subset)\n",
    "\n",
    "target_scaler = MinMaxScaler()\n",
    "Y_data_scaled = target_scaler.fit_transform(y_data_subset)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_data_scaled, Y_data_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "# DataLoader for batch processing\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define the neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_sizes[0]), nn.ReLU()]\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_sizes = [128, 128, 64]\n",
    "output_size = Y_train.shape[1]\n",
    "percentage_range=(88,95)\n",
    "# Instantiate the model\n",
    "model = NeuralNetwork(input_size, hidden_sizes, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "model.eval()  # set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_val_tensor)\n",
    "    # Inverse transform to get the predictions in the original scale\n",
    "    predictions_np = predictions.numpy()\n",
    "    predicted_parameters = target_scaler.inverse_transform(predictions_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.0648\n",
      "Epoch [200/1000], Loss: 0.0510\n",
      "Epoch [300/1000], Loss: 0.0491\n",
      "Epoch [400/1000], Loss: 0.0428\n",
      "Epoch [500/1000], Loss: 0.0446\n",
      "Epoch [600/1000], Loss: 0.0371\n",
      "Epoch [700/1000], Loss: 0.0417\n",
      "Epoch [800/1000], Loss: 0.0332\n",
      "Epoch [900/1000], Loss: 0.0402\n",
      "Epoch [1000/1000], Loss: 0.0337\n",
      "Predicted parameters for test data point with the minimum validation loss (Epoch 1000):\n",
      "[5.7960927e-01 1.0960482e+03 3.4411706e-02 4.7349989e-01 1.4338960e+03\n",
      " 4.6137509e+02 3.8619226e+02]\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [19997, 2000]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/sankoktas/Desktop/coeproject3/MAIN/MODELS/PART32_UPDATED.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sankoktas/Desktop/coeproject3/MAIN/MODELS/PART32_UPDATED.ipynb#W3sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m predicted_parameters_test \u001b[39m=\u001b[39m target_scaler\u001b[39m.\u001b[39minverse_transform(predictions_test_np)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sankoktas/Desktop/coeproject3/MAIN/MODELS/PART32_UPDATED.ipynb#W3sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m \u001b[39m# Calculate R-squared value\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/sankoktas/Desktop/coeproject3/MAIN/MODELS/PART32_UPDATED.ipynb#W3sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m r2_value \u001b[39m=\u001b[39m r2_score(y_data_subset, predicted_parameters_test)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sankoktas/Desktop/coeproject3/MAIN/MODELS/PART32_UPDATED.ipynb#W3sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mR-squared value on the test set: \u001b[39m\u001b[39m{\u001b[39;00mr2_value\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_regression.py:989\u001b[0m, in \u001b[0;36mr2_score\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, force_finite)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[39m@validate_params\u001b[39m(\n\u001b[1;32m    849\u001b[0m     {\n\u001b[1;32m    850\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m: [\u001b[39m\"\u001b[39m\u001b[39marray-like\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m     force_finite\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    869\u001b[0m ):\n\u001b[1;32m    870\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\":math:`R^2` (coefficient of determination) regression score function.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \n\u001b[1;32m    872\u001b[0m \u001b[39m    Best possible score is 1.0 and it can be negative (because the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[39m    -inf\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[39m=\u001b[39m _check_reg_targets(\n\u001b[1;32m    990\u001b[0m         y_true, y_pred, multioutput\n\u001b[1;32m    991\u001b[0m     )\n\u001b[1;32m    992\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    994\u001b[0m     \u001b[39mif\u001b[39;00m _num_samples(y_pred) \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_regression.py:99\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     66\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39m        correct keyword.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[1;32m    100\u001b[0m     y_true \u001b[39m=\u001b[39m check_array(y_true, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    101\u001b[0m     y_pred \u001b[39m=\u001b[39m check_array(y_pred, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    405\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    410\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [19997, 2000]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# Load the data\n",
    "x_data_path = 'MODEL_DATA/NEWDATA/newData_combined_FD.csv'  \n",
    "y_data_path = 'MODEL_DATA/NEWDATA/newData_expanded_realHardParam.csv' \n",
    "\n",
    "x_data = pd.read_csv(x_data_path)\n",
    "y_data = pd.read_csv(y_data_path)\n",
    "\n",
    "# Use a subset of the data for faster training\n",
    "subset_percentage = 0.99\n",
    "x_data_subset, _, y_data_subset, _ = train_test_split(x_data, y_data, test_size=1 - subset_percentage, random_state=42)\n",
    "\n",
    "# Data scaling\n",
    "input_scaler = MinMaxScaler()\n",
    "X_data_scaled = input_scaler.fit_transform(x_data_subset)\n",
    "\n",
    "target_scaler = MinMaxScaler()\n",
    "Y_data_scaled = target_scaler.fit_transform(y_data_subset)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_data_scaled, Y_data_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the remaining data into validation and test sets\n",
    "X_test, X_val, Y_test, Y_val = train_test_split(X_val, Y_val, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)  # Add this line for testing data\n",
    "Y_val_tensor = torch.tensor(Y_val, dtype=torch.float32)  # Add this line for validation targets\n",
    "\n",
    "# DataLoader for batch processing\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define the neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        layers = [nn.Linear(input_size, hidden_sizes[0]), nn.ReLU()]\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_sizes = [128, 128, 64]\n",
    "output_size = Y_train.shape[1]\n",
    "\n",
    "# Instantiate the model\n",
    "model = NeuralNetwork(input_size, hidden_sizes, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Initialize minimum validation loss and index\n",
    "min_val_loss = float('inf')\n",
    "min_val_loss_index = -1\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions_val = model(X_val_tensor)\n",
    "    loss_val = criterion(predictions_val, Y_val_tensor)\n",
    "\n",
    "# Check if current validation loss is the minimum\n",
    "if loss_val < min_val_loss:\n",
    "    min_val_loss = loss_val\n",
    "    min_val_loss_index = epoch\n",
    "\n",
    "# Evaluate on the test set and print the parameters for the data point with the minimum validation loss\n",
    "with torch.no_grad():\n",
    "    predictions_test = model(X_test_tensor)\n",
    "    predictions_test_np = predictions_test.numpy()\n",
    "    predicted_parameters_test = target_scaler.inverse_transform(predictions_test_np)\n",
    "\n",
    "# Print the parameters for the data point with the minimum validation loss\n",
    "if min_val_loss_index != -1:\n",
    "    parameters_min_loss = predicted_parameters_test[min_val_loss_index]\n",
    "    print(f\"Predicted parameters for test data point with the minimum validation loss (Epoch {min_val_loss_index + 1}):\")\n",
    "    print(parameters_min_loss)\n",
    "    print()\n",
    "else:\n",
    "    print(\"No minimum validation loss found.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared value on the test set: -0.5581\n"
     ]
    }
   ],
   "source": [
    "# Inverse transform the scaled predictions to get the original scale\n",
    "predicted_parameters_test = target_scaler.inverse_transform(predictions_test_np)\n",
    "\n",
    "# Ensure the shapes match for calculating R-squared\n",
    "y_data_test_subset = y_data_subset[:len(predicted_parameters_test)]\n",
    "\n",
    "# Calculate R-squared value\n",
    "r2_value = r2_score(y_data_test_subset, predicted_parameters_test)\n",
    "print(f\"R-squared value on the test set: {r2_value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
